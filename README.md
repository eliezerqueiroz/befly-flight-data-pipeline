# BeFly Data Engineering Challenge - Flight Data Pipeline

## VisÃ£o Geral

Este projeto implementa um pipeline de dados seguindo a arquitetura **MedalhÃ£o (Bronze, Silver, Gold)** utilizando **PySpark**. O objetivo Ã© processar dados histÃ³ricos de voos para gerar insights sobre pontualidade e cancelamentos.

O projeto foi desenvolvido com foco em:

- **Performance:** Uso de formato Parquet e avaliaÃ§Ã£o preguiÃ§osa (Lazy Evaluation).
- **Modularidade:** CÃ³digo organizado em funÃ§Ãµes e utilitÃ¡rios reutilizÃ¡veis.
- **Robustez:** Tratamento de tipos, nulos e logs detalhados.
- **OtimizaÃ§Ã£o Local:** ConfiguraÃ§Ãµes especÃ­ficas para execuÃ§Ã£o eficiente em ambiente local (ex: Apple M1).

---

## Arquitetura do Pipeline

O fluxo de dados segue a estrutura de Data Lake:

1. **IngestÃ£o (Bronze):**
    - Leitura dos CSVs brutos (`airlines`, `airports`, `flights`).
    - AplicaÃ§Ã£o de filtro de particionamento (MÃªs = 1) para otimizar processamento.
    - GravaÃ§Ã£o em formato **Parquet** (compressÃ£o Snappy).

2. **TransformaÃ§Ã£o (Silver):**
    - Limpeza de dados e *Type Casting* (garantia de schema).
    - CriaÃ§Ã£o de colunas derivadas (`FLIGHT_DATE`, `FLIGHT_STATUS`).
    - Enriquecimento via **Joins** com tabelas de dimensÃ£o (Companhias e Aeroportos).
    - Tratamento de ambiguidade em colunas de Origem/Destino.

3. **AgregaÃ§Ã£o (Gold):**
    - GeraÃ§Ã£o de mÃ©tricas de negÃ³cio (KPIs).
    - Tabelas finais: `airline_performance` (Pontualidade por cia) e `daily_summary` (VisÃ£o temporal).

---

## Estrutura do Projeto

```text
befly-flight-data-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # CSVs originais (Ignorados no Git)
â”‚   â”œâ”€â”€ bronze/       # Dados brutos em Parquet
â”‚   â”œâ”€â”€ silver/       # Dados limpos e enriquecidos
â”‚   â””â”€â”€ gold/         # AgregaÃ§Ãµes finais
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploracao_silver.ipynb   # ValidaÃ§Ã£o de schema e joins
â”‚   â””â”€â”€ exploracao_gold.ipynb     # ValidaÃ§Ã£o de regras de negÃ³cio
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ jobs/
â”‚   â”‚   â”œâ”€â”€ ingest_bronze.py      # Script de ingestÃ£o
â”‚   â”‚   â”œâ”€â”€ transform_silver.py   # Script de transformaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ aggregate_gold.py     # Script de agregaÃ§Ã£o
â”‚   â””â”€â”€ utils.py                  # ConfiguraÃ§Ãµes compartilhadas (SparkSession)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Como Executar

### PrÃ©-requisitos

- Python 3.8+
- Java 8 ou 11 (NecessÃ¡rio para o Spark)

### 1. ConfiguraÃ§Ã£o do Ambiente

```bash
# Clone o repositÃ³rio
git clone <url-do-repositorio>
cd befly-flight-data-pipeline

# Crie e ative o ambiente virtual
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Instale as dependÃªncias
pip install -r requirements.txt
```

### 2. ObtenÃ§Ã£o dos Dados

Devido ao tamanho, os dados brutos nÃ£o estÃ£o no repositÃ³rio.

1. Baixe o dataset "2015 Flight Delays and Cancellations" no Kaggle.
2. Extraia os arquivos `flights.csv`, `airlines.csv` e `airports.csv`.
3. Mova-os para a pasta: `data/raw/`.

### 3. ExecuÃ§Ã£o do Pipeline

Existem duas formas de executar o projeto:

#### OpÃ§Ã£o A: Orquestrador Automatizado (Recomendado)

Para simular um ambiente produtivo, foi desenvolvido um script Shell que gerencia a dependÃªncia entre as camadas (Bronze â†’ Silver â†’ Gold) e interrompe o fluxo imediatamente em caso de erro (**Fail Fast**).

1. DÃª permissÃ£o de execuÃ§Ã£o ao script (apenas na primeira vez):

   ```bash
   chmod +x run_pipeline.sh
   ```

2. Execute o pipeline completo:

   ```bash
   ./run_pipeline.sh
   ```

#### OpÃ§Ã£o B: ExecuÃ§Ã£o Manual (Passo a Passo)

Caso queira rodar ou debugar uma etapa especÃ­fica isoladamente:

```bash
# 1. IngestÃ£o Bronze
python src/jobs/ingest_bronze.py

# 2. TransformaÃ§Ã£o Silver
python src/jobs/transform_silver.py

# 3. AgregaÃ§Ã£o Gold
python src/jobs/aggregate_gold.py
```

---

## DecisÃµes TÃ©cnicas & OtimizaÃ§Ãµes

- **Leitura Otimizada:** O filtro `MONTH=1` Ã© aplicado imediatamente apÃ³s a leitura do CSV, reduzindo o volume de dados em memÃ³ria antes de qualquer transformaÃ§Ã£o pesada.
- **Gerenciamento de MemÃ³ria:** A `SparkSession` foi configurada via `src/utils.py` com limite de 2GB e 4 partiÃ§Ãµes para evitar *Out of Memory* em ambientes de desenvolvimento (ex: Laptops com 8GB RAM).
- **Logs:** UtilizaÃ§Ã£o da biblioteca `logging` em vez de `print` para garantir observabilidade profissional.
- **OrquestraÃ§Ã£o Local:** ImplementaÃ§Ã£o de um script Shell (`run_pipeline.sh`) que atua como um orquestrador linear simples. Ele garante a integridade do processo, impedindo que a camada Silver rode se a Bronze falhar, simulando o comportamento de dependÃªncia de tarefas de ferramentas como Apache Airflow.

---

## Arquitetura em Nuvem (AWS)

Para adaptar este pipeline local para um ambiente produtivo na AWS, a arquitetura seria modernizada da seguinte forma:

1. **Storage (Data Lake):**
    - SubstituiÃ§Ã£o do sistema de arquivos local pelo **Amazon S3**.
    - Buckets separados para camadas: `s3://befly-datalake-bronze`, `s3://...-silver`, `s3://...-gold`.

2. **Processamento (Compute):**
    - UtilizaÃ§Ã£o do **AWS Glue** (Serverless Spark) para executar os jobs Python. O Glue gerencia a escala automÃ¡tica dos workers, ideal para processar o dataset completo (5M+ linhas) sem preocupaÃ§Ã£o com infraestrutura.
    - Alternativamente, **EMR** poderia ser usado para cargas de trabalho contÃ­nuas e muito pesadas.

3. **CatÃ¡logo e Consumo:**
    - **AWS Glue Crawler** para catalogar automaticamente os dados gerados na camada Gold.
    - **Amazon Athena** para permitir que analistas faÃ§am consultas SQL diretamente nos arquivos Parquet no S3.

4. **OrquestraÃ§Ã£o:**
    - **Apache Airflow (MWAA)** para gerenciar a dependÃªncia entre as tarefas (sÃ³ rodar a Silver se a Bronze tiver sucesso) e agendamento diÃ¡rio.

Essa arquitetura garante escalabilidade horizontal, baixo custo de armazenamento (S3) e separaÃ§Ã£o entre computaÃ§Ã£o e storage.

---

## ğŸš€ Roadmap e Melhorias Futuras

Este projeto foi desenhado como um MVP para demonstrar competÃªncias em PySpark e Arquitetura de Dados. Em um cenÃ¡rio produtivo real, as seguintes evoluÃ§Ãµes seriam priorizadas:

- [ ] **Qualidade de Dados (Data Quality):**
    - IntegraÃ§Ã£o com **Great Expectations** ou **Soda** para validaÃ§Ã£o robusta de contratos de dados (ex: garantir que nÃ£o existam cÃ³digos de aeroporto invÃ¡lidos ou datas futuras) antes da promoÃ§Ã£o para a camada Silver.

- [ ] **Testes Automatizados:**
    - ImplementaÃ§Ã£o de testes unitÃ¡rios com **PyTest** para validar a lÃ³gica de funÃ§Ãµes isoladas (ex: cÃ¡lculo de atrasos) e *mocks* de DataFrames para testes de integraÃ§Ã£o sem dependÃªncia de dados externos.

- [ ] **ModernizaÃ§Ã£o do Formato de Armazenamento:**
    - MigraÃ§Ã£o de Parquet para **Delta Lake**. Isso habilitaria:
        - TransaÃ§Ãµes ACID (garantia de integridade em escritas concorrentes).
        - Suporte a operaÃ§Ãµes de `MERGE` (Upserts) para processar apenas dados novos (Incremental Load) em vez de reprocessamento total.
        - *Time Travel* para auditoria e rollback de dados.

- [ ] **ContainerizaÃ§Ã£o:**
    - CriaÃ§Ã£o de um `Dockerfile` para encapsular o ambiente (Java + Spark + Python), garantindo reprodutibilidade exata em qualquer sistema operacional e facilitando o deploy em Kubernetes ou AWS ECS.

- [ ] **CI/CD:**
    - ConfiguraÃ§Ã£o de pipeline no **GitHub Actions** para execuÃ§Ã£o automÃ¡tica de linter (`ruff`/`black`), testes unitÃ¡rios e deploy de infraestrutura (Terraform) a cada Pull Request.